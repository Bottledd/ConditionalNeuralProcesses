import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np
import tensorflow.keras as keras
from tensorflow.keras import layers

def load_data():
    data, info = tfds.load(
        "omniglot",
        split='train+test',
        shuffle_files=True,
        as_supervised=True,
        with_info=True
    )
    return data, info


def format_data(image, label):
    image = tf.image.resize(image, [28, 28])
    image = tf.image.rgb_to_grayscale(image)
    image = tf.cast(image, tf.float32) / 255.0
    return image, label


def rotate_and_new_class(image, label):
    image = tf.image.rot90(image)
    label = 1600 + tf.squeeze(label)
    return image, label


def training_filter(label, allowed_labels):
    is_allowed = tf.equal(allowed_labels, tf.cast(label, tf.float32))
    reduced = tf.reduce_sum(tf.cast(is_allowed, tf.float32))
    return tf.greater(reduced, tf.constant(0.))

def get_N_show_labels(image, label):
    pass

def build_data(data):
    """
    :param data: tfds object containing full dataset
    :return: train, test data which is randomly chosen by class
    """
    AUTOTUNE = tf.data.experimental.AUTOTUNE
    data = data.map(format_data, num_parallel_calls=AUTOTUNE)
    # since we will randomly select 1200 classes and 20 examples per class

    # randomly sample 1200 classes to use for training
    train_classes = np.random.choice(np.arange(1600), size=1200, replace=False)
    test_classes = np.setdiff1d(np.arange(1600), train_classes)

    # make into tensors
    train_classes = tf.constant(train_classes, dtype=tf.float32)
    test_classes = tf.constant(test_classes, dtype=tf.float32)

    # filter the dataset
    train_data = data.filter(lambda image, label: training_filter(label, allowed_labels=train_classes))
    test_data = data.filter(lambda image, label: training_filter(label, allowed_labels=test_classes))

    # rotate training set images and make them new classes
    # for labels take original class and add
    rotated_train_data = train_data.map(rotate_and_new_class, num_parallel_calls=AUTOTUNE)

    train_data = train_data.concatenate(rotated_train_data)

    return train_data, test_data, train_classes


def training_data_per_loop(training_data, train_classes, N_WAY=5, M_SHOT=1, batch_size=5):
    AUTOTUNE = tf.data.experimental.AUTOTUNE

    # sample N classes
    sampled_classes = np.random.choice(train_classes, N_WAY, replace=False)
    sampled_classes = tf.constant(sampled_classes, dtype=tf.float32)

    N_WAY_DATA = training_data.filter(lambda image, label: training_filter(label, allowed_labels=sampled_classes))

    # sort the dataset by label, we use this to select M images for training
    N_WAY_DATA = N_WAY_DATA.cache()
    N_WAY_DATA = N_WAY_DATA.shuffle(N_WAY*20)
    N_WAY_DATA = N_WAY_DATA.batch(batch_size)
    N_WAY_DATA = N_WAY_DATA.prefetch(AUTOTUNE)

    return N_WAY_DATA


def give_label_for_context():
    pass



if __name__ == '__main__':
    data, info = load_data()
    train, test, training_classes = build_data(data)
    loop_data = training_data_per_loop(train, training_classes)
